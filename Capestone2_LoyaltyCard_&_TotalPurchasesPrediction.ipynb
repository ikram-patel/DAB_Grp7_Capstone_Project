{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi Mart data for Frequency and Revenue Analysis**\n",
        "\n",
        "Frequency and Revenue Analysis is an estimate of all the future profits to be accumulated from a relationship with a given customer. It is used in the business to measure the performance of retention strategies and to provide insights into how much should be spent in customer acquisition."
      ],
      "metadata": {
        "id": "cyn0nI3Q50AU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Dictionary**"
      ],
      "metadata": {
        "id": "dXA1ewyZxoFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**: To understand and gain insights from an E-Commerce dataset by performing various exploratory data analyses, data visualization, and data modelling.<br>\n",
        "<br>\n",
        "**Dataset Columns:**\n",
        "\n",
        "**CustomerID** : Unique customer ID<br>\n",
        "**first purchase date** : It refers to the date when a customer or user made their initial purchase or transaction with the organization.<br>\n",
        "**last purchase date** : It refers to the date when a customer or user made their most recent purchase or transaction with the organization.<br>\n",
        "**total purchases** : It is the count or sum of all purchases made by a customer or user with the organization.<br>\n",
        "**total revenue** : It is the sum of all revenue generated from customer or user transactions with the organization.<br>\n",
        "**referral source** : It provides information about how individuals found out about the products or services.<br>\n",
        "**churn indicator** : This is a binary flag that indicates whether a customer or user has churned (i.e., stopped using the products or services) or is still an active customer. Typically, a value of 1 or \"Yes\" is used to indicate churn, while 0 or \"No\" is used to indicate an active customer.<br>\n",
        "**discount used** : It provides information about whether a discount was utilized for a specific purchase or order.<br>\n",
        "**product category** : It classifies products into specific categories or groups based on their characteristics or purpose.<br> **responsetolastcampaign** : This indicates whether and how a customer or user responded to the most recent marketing campaign.<br>\n",
        "**feedbackscore** : It represents a numeric score or rating provided by customers or users as feedback for a product, service, or experience.<br>\n",
        "**preferredpaymentmethod** : It provides information about the customer's preferred way to make payments.<br>\n",
        "**supportticketsraised** : It represents the number of customer or user support tickets that have been opened or raised by individuals seeking assistance, reporting issues, or making inquiries.<br>\n",
        "**hasloyaltycard** : This is a binary indicator that shows whether a customer or user possesses a loyalty card with the organization.<br>\n",
        "**frequency** : The frequency column represents how often a customer or user interacts with the organization, such as making purchases, engaging with the services, or participating in activities. The frequency column is based on the first purchase date and the last purchase date period.\n"
      ],
      "metadata": {
        "id": "Iq4R1fYL63n4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWTslCvY4iUU"
      },
      "outputs": [],
      "source": [
        "# importing the necessary  libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import itertools\n",
        "import warnings\n",
        "import plotly.offline as py\n",
        "\n",
        "# Initialize Plotly in notebook mode\n",
        "py.init_notebook_mode(connected=True)\n",
        "\n",
        "# Enable inline plotting for matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the default Plotly renderer to 'colab'\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "# Ignore warnings to prevent them from being displayed\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Mount google drive to access data set\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# reading the data\n",
        "data = pd.read_csv('Customer_Lifetime_Value_Dataset.csv')\n",
        "\n",
        "# printing the head of the data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HMq86myNXQcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting an overview of the data\n",
        "data.info()"
      ],
      "metadata": {
        "id": "FMiCzVcFjpGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing an overview of the dataset\n",
        "print (\"Rows     : \" ,data.shape[0])\n",
        "print (\"Columns  : \" ,data.shape[1])\n",
        "print (\"\\nFeatures : \\n\" ,data.columns.tolist())\n",
        "print (\"\\nUnique values :  \\n\",data.nunique())"
      ],
      "metadata": {
        "id": "Z5hFiqxK6KEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values\n",
        "missing_data = data.isnull()\n",
        "for column in missing_data.columns.values.tolist():\n",
        "    print(column)\n",
        "    print (missing_data[column].value_counts())\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "1A-oPprvj3ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The dataset consists of 10k records & no missing values in them."
      ],
      "metadata": {
        "id": "GWL1YWuKOPv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for duplicate values\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "JXmU1U1lj40i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the columns have duplicate values but they are justified according to a retailer dataset."
      ],
      "metadata": {
        "id": "bQgRsVUQNlx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Statistical Analysis**"
      ],
      "metadata": {
        "id": "mP1J4kQ5BihN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting an overall statistical analysis of the data by using .describe()\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "dgf_SW5jn_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One-Hot Encoding**"
      ],
      "metadata": {
        "id": "C3EH2lmbyFZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Identify categorical columns\n",
        "# categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "columns_to_encode = ['referralsource', 'responsetolastcampaign', 'preferredpaymentmethod']\n",
        "\n",
        "# Perform one-hot encoding for all categorical columns\n",
        "onehot_data = pd.get_dummies(data, columns=columns_to_encode)\n",
        "\n",
        "# # Display the encoded DataFrame\n",
        "# print(onehot_data)"
      ],
      "metadata": {
        "id": "45_Hd4r2j7Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_data.columns"
      ],
      "metadata": {
        "id": "mM4O777_z9bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_data.head()"
      ],
      "metadata": {
        "id": "JftpjkrO1BcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correlationship with One-Hot Encoded Columns**"
      ],
      "metadata": {
        "id": "tyL40IDUyK6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = onehot_data.corr()\n",
        "# importing the necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# formation of correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Creating a heatmap\n",
        "plt.figure(figsize=(40, 30))\n",
        "sns.heatmap(matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhHoPzw_LU3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**"
      ],
      "metadata": {
        "id": "y3MZ1zpfyWcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "# Dropping non-numeric columns\n",
        "X = onehot_data.drop([target_column, 'customerid', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'avgpurchasevalue', 'avgtimebetweenpurchases', 'supportticketsraised',\n",
        "                      'hasloyaltycard'], axis=1)\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Converting non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2_score(y_test, y_pred_rf)"
      ],
      "metadata": {
        "id": "ROjGncKDLPjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "ByoAJ9Xp2TW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression**"
      ],
      "metadata": {
        "id": "o_E585urycjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "# Dropping non-numeric and target columns\n",
        "X = onehot_data.drop([target_column, 'customerid', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'avgpurchasevalue', 'avgtimebetweenpurchases', 'supportticketsraised',\n",
        "                      'hasloyaltycard'], axis=1)\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Converting non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing the Linear Regression model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = linear_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "# mean_squared_error(y_test, y_pred)\n",
        "r2_score(y_test, y_pred_lr)"
      ],
      "metadata": {
        "id": "loNp1blds_7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred_lr)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "E956-3f310UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Importance**"
      ],
      "metadata": {
        "id": "T8tRik7RzmRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_numeric.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance DataFrame\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Plot the top N feature importances\n",
        "# top_n = 10  # You can adjust this based on your preference\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
        "# plt.title(f'Top {top_n} Feature Importances')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SzZbZzu3zqhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PCA**"
      ],
      "metadata": {
        "id": "2q_uBuVM4qUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize PCA with the desired number of components\n",
        "n_components = 3  # Number of principal components to keep\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit PCA to your data and transform the data to the new feature space\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Create a DataFrame for the transformed data\n",
        "X_pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
        "\n",
        "# Display the transformed data\n",
        "print(\"Transformed Data (First 5 rows):\")\n",
        "print(X_pca_df.head())"
      ],
      "metadata": {
        "id": "TBRhSVpQ4syX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'pca' is your trained PCA model\n",
        "# Access the loadings (coefficients) for each principal component\n",
        "loadings = pca.components_\n",
        "\n",
        "# Retrieve the loadings for PC1\n",
        "loadings_pc1 = loadings[0]  # Assuming PC1 is the first component\n",
        "\n",
        "# Create a DataFrame to display the loadings\n",
        "loadings_df = pd.DataFrame(data=loadings_pc1, index=X.columns, columns=['Loading_PC1'])\n",
        "\n",
        "# Display the loadings for PC1\n",
        "print(\"Loadings for PC1:\")\n",
        "print(loadings_df)\n"
      ],
      "metadata": {
        "id": "pLLRh-VVHOY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest with PCA and Feature Importance Columns**"
      ],
      "metadata": {
        "id": "SWpNZDGe4bH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "features = ['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore', 'discountsused', 'churnindicator', 'referralsource_Online advertisements', 'preferredpaymentmethod_debit card', 'referralsource_Influencer endorsements', 'preferredpaymentmethod_credit card', 'referralsource_Social media promotions', 'referralsource_Word of mouth', 'referralsource_Email campaigns', 'responsetolastcampaign_opened mail', 'referralsource_In-store promotions', 'preferredpaymentmethod_cash', 'preferredpaymentmethod_apple pay', 'responsetolastcampaign_ignored', 'preferredpaymentmethod_paypal']\n",
        "# Dropping non-numeric columns\n",
        "X = onehot_data[features]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2_score(y_test, y_pred_rf)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(r2_score(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "GzT3YIU42qTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "features = ['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore', 'discountsused', 'churnindicator']\n",
        "# Dropping non-numeric columns\n",
        "X = onehot_data[features]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2_score(y_test, y_pred_rf)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(r2_score(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "CMj7_05P5_Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression using PCA and Feature Importance Columns**"
      ],
      "metadata": {
        "id": "orygrdE54nff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "features = ['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore', 'discountsused', 'churnindicator', 'referralsource_Online advertisements', 'preferredpaymentmethod_debit card', 'referralsource_Influencer endorsements', 'preferredpaymentmethod_credit card', 'referralsource_Social media promotions', 'referralsource_Word of mouth', 'referralsource_Email campaigns', 'responsetolastcampaign_opened mail', 'referralsource_In-store promotions', 'preferredpaymentmethod_cash', 'preferredpaymentmethod_apple pay', 'responsetolastcampaign_ignored', 'preferredpaymentmethod_paypal']\n",
        "# Dropping non-numeric columns\n",
        "X = onehot_data[features]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Converting non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing the Linear Regression model\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = linear_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "# mean_squared_error(y_test, y_pred)\n",
        "r2_score(y_test, y_pred_lr)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(r2_score(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "UO4u_yue45KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "NjZip5m52ht0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "target_column = 'totalpurchases'\n",
        "features = ['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore', 'discountsused', 'churnindicator', 'referralsource_Online advertisements', 'preferredpaymentmethod_debit card', 'referralsource_Influencer endorsements', 'preferredpaymentmethod_credit card', 'referralsource_Social media promotions', 'referralsource_Word of mouth', 'referralsource_Email campaigns', 'responsetolastcampaign_opened mail', 'referralsource_In-store promotions', 'preferredpaymentmethod_cash', 'preferredpaymentmethod_apple pay', 'responsetolastcampaign_ignored', 'preferredpaymentmethod_paypal']\n",
        "# Dropping non-numeric columns\n",
        "X = onehot_data[features]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize LazyRegressor\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "\n",
        "# Fit LazyRegressor\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print model performance\n",
        "print(models)"
      ],
      "metadata": {
        "id": "2VtIeH3V2ktM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting**"
      ],
      "metadata": {
        "id": "-9hAkwopk9Gm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suXdf8KyUQdB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Assuming 'totalpurchases' is your target column\n",
        "target_column = 'totalpurchases'\n",
        "\n",
        "# Drop non-numeric and target columns\n",
        "X = onehot_data.drop([target_column, 'customerid', 'firstpurchasedate', 'lastpurchasedate', 'productcategory','supportticketsraised','hasloyaltycard','avgpurchasevalue',\n",
        "               'avgtimebetweenpurchases'], axis=1)\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Convert non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values (resulting from non-numeric conversion) with some appropriate value or strategy\n",
        "X_numeric.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate R-squared value\n",
        "r2_encoded = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared value using Gradient Boosting Regressor model:\", r2_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grid Search for 'totalpurchases'**"
      ],
      "metadata": {
        "id": "sJtfMCB3Uhw_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PWck4or3Z0F"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Assuming 'totalpurchases' is your target column\n",
        "target_column = 'totalpurchases'\n",
        "\n",
        "# Drop non-numeric and target columns\n",
        "X = onehot_data[['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore', 'discountsused', 'churnindicator']]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Convert non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values (resulting from non-numeric conversion) with some appropriate value or strategy\n",
        "X_numeric.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search through\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of boosting stages to be run\n",
        "    'learning_rate': [0.05, 0.1, 0.2],  # Boosting learning rate\n",
        "    'max_depth': [3, 4, 5]  # Maximum depth of the individual regression estimators\n",
        "}\n",
        "\n",
        "# Initialize the GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best parameters found:\", best_params)\n",
        "print(\"Best R-squared score found:\", best_score)\n",
        "\n",
        "# Use the best estimator for prediction\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_pred_grid = best_estimator.predict(X_test)\n",
        "\n",
        "# Calculate R-squared value using the best estimator\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "print(\"R-squared value using the best estimator from GridSearchCV:\", r2_grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming 'totalpurchases' is your target column\n",
        "target_column = 'totalpurchases'\n",
        "\n",
        "# Drop non-numeric and target columns\n",
        "X = onehot_data[['frequency', 'tenure', 'recency', 'totalrevenue', 'feedbackscore']]\n",
        "y = onehot_data[target_column]\n",
        "\n",
        "# Convert non-numeric columns to numeric\n",
        "X_numeric = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values (resulting from non-numeric conversion) with some appropriate value or strategy\n",
        "X_numeric.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate = 0.05, max_depth = 3)\n",
        "\n",
        "# Fit the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate R-squared value\n",
        "r2_encoded = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared value using Gradient Boosting Regressor model:\", r2_encoded)"
      ],
      "metadata": {
        "id": "J7MjyjhnUmCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **'hasloyalty' Analysis**"
      ],
      "metadata": {
        "id": "upXcqKPkt21Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/DAB303/group project/project-4/Customer_Lifetime_Value_Dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0lL83hX-njrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Label Encoding**"
      ],
      "metadata": {
        "id": "FMcS99IiGMnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Select the columns to be label encoded\n",
        "columns_to_encode = ['referralsource', 'responsetolastcampaign', 'preferredpaymentmethod', 'hasloyaltycard']\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Create a new DataFrame 'encoded_data' to store the label-encoded data\n",
        "encoded_data = df.copy()\n",
        "\n",
        "# Iterate over the selected columns and apply label encoding\n",
        "for column in columns_to_encode:\n",
        "    # Fit and transform the column\n",
        "    encoded_data[column] = label_encoder.fit_transform(encoded_data[column])\n",
        "\n",
        "encoded_data.head()"
      ],
      "metadata": {
        "id": "OAD5n_Egt2Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Label Encoded Correlation**"
      ],
      "metadata": {
        "id": "qDcnKahKGRpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data.corr()"
      ],
      "metadata": {
        "id": "kzpIp_ZPuTd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calculate the correlation matrix\n",
        "correlation_matrix = encoded_data.corr()\n",
        "\n",
        "# Set the threshold values\n",
        "positive_threshold = 0.0999999\n",
        "negative_threshold = -0.0999999\n",
        "\n",
        "# Find correlation values greater than 0.0999999\n",
        "positive_correlations = correlation_matrix[correlation_matrix > positive_threshold].stack()\n",
        "\n",
        "# Find correlation values lesser than -0.099999\n",
        "negative_correlations = correlation_matrix[correlation_matrix < negative_threshold].stack()\n",
        "\n",
        "# Display the results\n",
        "print(\"Correlation Values Greater than 0.0999999:\")\n",
        "print(positive_correlations)\n",
        "\n",
        "# print(\"\\nCorrelation Values Lesser than -0.099999:\")\n",
        "print(negative_correlations)"
      ],
      "metadata": {
        "id": "VRraE-Ezu_sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data.columns"
      ],
      "metadata": {
        "id": "sdpZYMvpxSQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lazy Predict and Random Forest Classifier**"
      ],
      "metadata": {
        "id": "TKJ1VXTEGYW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "target_column = 'hasloyaltycard'\n",
        "# features = ['hasloyaltycard', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'customerid']\n",
        "# Dropping non-numeric columns\n",
        "y = encoded_data[target_column]\n",
        "X = encoded_data.drop(['hasloyaltycard', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'supportticketsraised'], axis=1)\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize LazyRegressor\n",
        "classy = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "\n",
        "# Fit LazyRegressor\n",
        "models, predictions = classy.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print model performance\n",
        "print(models)"
      ],
      "metadata": {
        "id": "rFPQ_W0yv99h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "target_column = 'hasloyaltycard'\n",
        "\n",
        "# Dropping non-numeric columns\n",
        "X_best = encoded_data.drop(['hasloyaltycard', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'supportticketsraised', 'customerid'], axis=1)\n",
        "y_best = encoded_data[target_column]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_rfc = rfc.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
        "\n",
        "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
        "accuracy = accuracy_score(y_test, y_pred_rfc)\n",
        "precision = precision_score(y_test, y_pred_rfc)\n",
        "recall = recall_score(y_test, y_pred_rfc)\n",
        "f1 = f1_score(y_test, y_pred_rfc)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Jt81pZ_Uybi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grid Search**"
      ],
      "metadata": {
        "id": "bcqZMevWin-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [300, 400, 350, 250],\n",
        "    'max_depth': [35, 40, 45, 50],\n",
        "    'min_samples_split': [25, 27, 30,33],\n",
        "    'min_samples_leaf': [8, 10, 11, 12]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with the specified parameter grid and the classifier\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the testing data using the best model\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "ERYNvuzvQ5Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance values in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance DataFrame\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "id": "U8PizbWjj_9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OneHot Encoded**"
      ],
      "metadata": {
        "id": "U-vN1hHYGoCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Select the columns to be one-hot encoded\n",
        "columns_to_onehot_encode = ['referralsource', 'responsetolastcampaign', 'preferredpaymentmethod']\n",
        "\n",
        "# Create a new DataFrame 'onehot_data' to store the one-hot encoded data\n",
        "onehot = df.copy()\n",
        "\n",
        "# Iterate over the selected columns and apply one-hot encoding\n",
        "for column in columns_to_onehot_encode:\n",
        "    # Apply one-hot encoding\n",
        "    encoded_column = pd.get_dummies(onehot[column], prefix=column)\n",
        "\n",
        "    # Concatenate the one-hot encoded column with the original DataFrame\n",
        "    onehot = pd.concat([onehot, encoded_column], axis=1)\n",
        "\n",
        "    # Drop the original column\n",
        "    onehot.drop(column, axis=1, inplace=True)\n",
        "\n",
        "# Label encode the 'hasloyaltycard' column\n",
        "label_encoder = LabelEncoder()\n",
        "onehot['hasloyaltycard'] = label_encoder.fit_transform(onehot['hasloyaltycard'])\n",
        "\n",
        "onehot.head()"
      ],
      "metadata": {
        "id": "DwM15dKSHY2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot.columns"
      ],
      "metadata": {
        "id": "FviRfZMIIAQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest One-Hot**"
      ],
      "metadata": {
        "id": "sAaeclgCITds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "target_column = 'hasloyaltycard'\n",
        "\n",
        "# Dropping non-numeric columns\n",
        "X = onehot.drop(['hasloyaltycard', 'firstpurchasedate', 'lastpurchasedate', 'productcategory', 'supportticketsraised'], axis=1)\n",
        "y = onehot[target_column]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_trainoh, X_testoh, y_trainoh, y_testoh = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rfcl = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfcl.fit(X_trainoh, y_trainoh)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_rfcl = rfcl.predict(X_testoh)\n",
        "\n",
        "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
        "accuracy = accuracy_score(y_testoh, y_pred_rfcl)\n",
        "precision = precision_score(y_testoh, y_pred_rfcl)\n",
        "recall = recall_score(y_testoh, y_pred_rfcl)\n",
        "f1 = f1_score(y_testoh, y_pred_rfcl)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "MiWSCZ3oIGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Imbalance**"
      ],
      "metadata": {
        "id": "3ncy6_5ASxrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **K-Fold Cross Validation**"
      ],
      "metadata": {
        "id": "KxlB0eG_SsQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define the scoring metrics\n",
        "scoring = {'precision': make_scorer(precision_score),\n",
        "           'recall': make_scorer(recall_score),\n",
        "           'f1_score': make_scorer(f1_score),\n",
        "           'accuracy': make_scorer(accuracy_score)}\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "k = 5\n",
        "cv_results = cross_validate(rfc, X_best, y_best, cv=k, scoring=scoring)\n",
        "\n",
        "# Print the cross-validation results\n",
        "print(\"Cross-Validation Scores:\")\n",
        "for metric in scoring:\n",
        "    mean_cv_score = cv_results[f\"test_{metric}\"].mean()\n",
        "    print(f\"{metric.capitalize()}:\")\n",
        "    print(\"  Mean:\", mean_cv_score)"
      ],
      "metadata": {
        "id": "sCFV-4AbS2V1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}